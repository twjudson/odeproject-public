<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                                 -->
<!--                                                               -->
<!--    Ordinary Differential Equations Project                    -->
<!--                                                               -->
<!-- Copyright (C) 2013-2025 Thomas W. Judson                      -->
<!-- See the file COPYING for copying conditions.                  -->

<section xml:id="firstlook04" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Analyzing Equations Numerically</title>
    <objectives>
        <ul>
        
            <li><p>To understand that numerical algorithms such as <term>Euler's method</term> allow the approximation of solutions to the initial value problems and that there are more efficient algorithms than Euler's method such as those algorithms that use the <term>Runge-Kutta methods</term>.</p></li>

            <li><p>To understand that <term>Taylor's Theorem</term> is a very useful tool for studying differential equations.</p></li>

            <li><p>To understand that error analysis of the rate of convergence is very important for any numerical algorithm.</p></li>

        </ul>
    </objectives>

    <introduction>
        <p>Just as numerical algorithms are useful when finding the roots of polynomials, numerical methods will prove very useful in our study of ordinary differential equations. Consider the polynomial <m>f(x) = x^2 - 2</m>.  We do not need a numerical algorithm to see that the roots of this polynomial are <m>x = \sqrt{2}</m> and <m>x = - \sqrt{2}</m>. However, a numerical method such as the Newton-Raphson Algorithm is very useful for approximating <m>\sqrt{2}</m> as a decimal.<fn>See any calculus text for a description of the Newton-Raphson Algorithm.</fn> Similarly, it may be easier to generate a numerical solution for differential equations if our goal is simply to plot a solution. In addition, there will be differential equations for which it is impossible to find a solution in terms of elementary functions such as polynomials, trigonometric functions, and exponential functions.</p>
    </introduction>
        
    <subsection xml:id="firstlook04-subsection-eulers-method">
        <title>Euler's Method</title>

        <p>Suppose that we wish to solve the initial value problem
            <mdn>
                <mrow xml:id="firstlook04-equation-euler-1">y' &amp;= f(t, y) = y + t, </mrow>
                <mrow xml:id="firstlook04-equation-euler-2">y(0) &amp;= 1. </mrow>
            </mdn>
        The equation <m>y' = y + t</m> is not separable, which currently is the only analytic technique at our disposal. However, we can try to find a numerical approximation for the solution. A numerical approximation is simply a table (possibly very large) of <m>t</m> and <m>y</m> values.</p>

        <p>We will attempt to find a numerical solution for <xref ref="firstlook04-equation-euler-1" /><ndash /><xref ref="firstlook04-equation-euler-2" /> on the interval <m>[0, 1]</m>. Even with the use of a computer, we cannot approximate the solution at every single point on an interval. For the initial value problem
            <md>
                <mrow>y' &amp;= f(t, y)</mrow>
                <mrow>y(t_0) &amp;= y_0, </mrow>
            </md>
        we might be able to find approximations at <m>a = t_0, t_1, t_2, \ldots, t_N = b</m> in <m>[a, b]</m> at best. If we choose <m>t_1, t_2, \ldots, t_N</m> to be equally spaced on <m>[a, b]</m>, we can write
            <me>t_k = t_0 + kh,</me>
        where <m>h = 1/N</m> and <m>k = 1, 2, \ldots, N</m>.  We say that <m>h</m> is the <term>step size</term><idx><h>Euler's method</h><h>step size</h></idx> for our approximation.</p>

        <p>Given an approximation <m>Y_k</m> for the solution <m>y_k = y(t_k)</m>, the question is how to find an approximate solution <m>Y_{k+1}</m> at <m>t_{k+1}</m>. To generate the second approximation, we will construct a tangent line to the solution at <m>y(t_0) = y_0</m>.  If we use the slope of the solution curve at <m>t_0</m>, then
            <me>y'(t_0) = f(t_0, y_0).</me>
        Making use of the fact that 
            <me>\frac{y(t_0 + h) - y(t_0)}{h} \approx  y'(t_0, y(t_0)) = y'(t_0, y_0)</me>
        or equivalently
            <me>y(t_0 + h) = y(t_1) \approx y(t_0) + h y'(t_0, y_0),</me>
        the estimate for our solution at <m>t_1 = t_0 + h</m> is
            <me>Y_1 = Y_0 + h f(t_0, Y_0).</me>
        Similarly, the approximation at <m>t_2 = t_0 + 2h</m> will be
            <me>Y_2 = Y_1 + h f(t_1, Y_1).</me>
        Our general algorithm is
            <me>Y_{k+1} = Y_k + h f(t_k, Y_k).</me>
        The idea is to compute tangent lines at each step and use this information to get our next approximation.</p>

        <p>The algorithm that we have described is known as <term>Euler's method</term><idx>Euler's method</idx>. Let us estimate a solution to <xref ref="firstlook04-equation-euler-1" /><ndash /><xref ref="firstlook04-equation-euler-2" /> on the interval <m>[0, 1]</m> with step size <m>h = 0.1</m>. Since <m>y(0) = 1</m>, we can make our first approximation exact,
            <me>Y_0 = y(0) = 1.</me>
        To generate the second approximation, we will construct a tangent line to the solution at <m>y(0) = 1</m>. If we use the slope of the solution curve at <m>t_0 = 0</m>, 
            <me>y'(0) = f(y(0), 0) = y(0) + 0 = 1 + 0 = 1,</me>
        and make use of the fact that 
            <me>\frac{y(h) - y(0)}{h} \approx y'(0, y(0)) \qquad \text{or} \qquad y(h) \approx y(0) + hy'(0, y(0)),</me>
        the estimate for our solution at <m>t = 0.1</m> is
            <md>
                <mrow>Y_1 &amp; = Y_0 + h f(t_0, Y_0)</mrow>
                <mrow>&amp; = Y_0 + h[Y_0 + t_0]</mrow>
                <mrow>&amp; = 1 + (0.1) [1 + 0]</mrow>
                <mrow>&amp; = 1.1000.</mrow>
            </md>
        Similarly,  the approximation at <m>t = 0.2</m> will be
            <md>
                <mrow>Y_2 &amp; = Y_1 + h f(t_1, Y_1)</mrow>
                <mrow>&amp; = Y_1 + h[Y_1 + t_1]</mrow>
                <mrow>&amp; = 1.1000 + (0.1) [1.1000 + 0.1]</mrow>
                <mrow>&amp; = 1.2200.</mrow>
            </md>
        Our general algorithm is
            <me>Y_{k+1} = Y_k + h f(t_k, Y_k) = Y_k + h[Y_k + t_k] = (1.1) Y_k + (0.01)k.</me></p>

        <p>The initial value problem <xref ref="firstlook04-equation-euler-1" /><ndash /><xref ref="firstlook04-equation-euler-2" /> is, in fact, solvable analytically with solution <m>y(t) = 2e^t - t - 1</m>. We can compare our approximation to the exact solution in <xref ref="firstlook04-table-euler-approximation" />. We can also see graphs of the approximate and exact solutions in <xref ref="firstlook04-figure-euler-approximation" />. Notice that the error grows as we get further away from our initial value. In fact, the graph of the approximation for <m>h = 0.001</m> is obscured by the graph of the exact solution. In addition, a smaller step size gives us a more accurate approximation (<xref ref="firstlook04-table-euler-approximation-step-size" />).</p>

        <table xml:id="firstlook04-table-euler-approximation">
            <title>Euler's approximation for <m>y' = y + t</m></title>
            <tabular halign="center" top="medium">
                <row bottom="medium">
                    <cell><m>k</m></cell><cell><m>t_k</m></cell><cell><m>Y_k</m></cell><cell><m>y_k</m></cell><cell><m>|y_k - Y_k|</m></cell><cell>Percent Error</cell>
                </row>
                <row>
                    <cell>0</cell><cell>0.0</cell><cell>1.0000</cell><cell>1.0000</cell><cell>0.0000</cell><cell>0.00%</cell>
                </row>
                <row>
                    <cell>1</cell><cell>0.1</cell><cell>1.1000</cell><cell>1.1103</cell><cell>0.0103</cell><cell>0.93%</cell>
                </row> 
                <row>
                    <cell>2</cell><cell>0.2</cell><cell>1.2200</cell><cell>1.2428</cell><cell>0.0228</cell><cell>1.84%</cell>
                </row>
                <row>
                    <cell>3</cell><cell>0.3</cell><cell>1.3620</cell><cell>1.3997</cell><cell>0.0377</cell><cell>2.69%</cell>
                </row>
                <row>
                    <cell>4</cell><cell>0.4</cell><cell>1.5282</cell><cell>1.5836</cell><cell>0.0554</cell><cell>3.50%</cell>
                </row>
                <row>
                    <cell>5</cell><cell>0.5</cell><cell>1.7210</cell><cell>1.7974</cell><cell>0.0764</cell><cell>4.25%</cell>
                </row>
                <row>
                    <cell>6</cell><cell>0.6</cell><cell>1.9431</cell><cell>2.0442</cell><cell>0.1011</cell><cell>4.95%</cell>
                </row>
                <row bottom="medium">
                    <cell>10</cell><cell>1.0</cell><cell>3.1875</cell><cell>3.4366</cell><cell>0.2491</cell><cell>7.25%</cell>
                </row>
            </tabular>      
        </table>

        <figure xml:id="firstlook04-figure-euler-approximation">
             <image width="60%">
                    <xi:include href="prefigure/firstlook04-euler-approximation.xml"/>
              </image>
            <caption>Euler's approximation for <m>y' = y + t</m></caption>
        </figure>

        <table xml:id="firstlook04-table-euler-approximation-step-size">
            <title>Step sizes for Euler's approximation</title>
            <tabular halign="center" top="medium">
                <row bottom="medium">
                    <cell><m>t_k</m></cell><cell><m>h = 0.1</m></cell><cell><m>h = 0.02</m></cell><cell><m>h = 0.001</m></cell><cell>Exact Solution</cell>
                </row>
                <row>
                    <cell>0.1</cell><cell>1.1000</cell><cell>1.1082</cell><cell>1.1102</cell><cell>1.1103</cell>
                </row>
                <row>
                    <cell>0.2</cell><cell>1.2200</cell><cell>1.2380</cell><cell>1.2426</cell><cell>1.2428</cell>
                </row>
                <row>
                    <cell>0.3</cell><cell>1.3620</cell><cell>1.3917</cell><cell>1.3993</cell><cell>1.3997</cell>
                </row>
                <row>
                    <cell>0.4</cell><cell>1.5282</cell><cell>1.5719</cell><cell>1.5831</cell><cell>1.5836</cell>
                </row>
                <row>
                    <cell>0.5</cell><cell>1.7210</cell><cell>1.7812</cell><cell>1.7966</cell><cell>1.7974</cell>
                </row>
                <row>
                    <cell>0.6</cell><cell>1.9431</cell><cell>2.0227</cell><cell>2.0431</cell><cell>2.0442</cell>
                </row>
                <row>
                    <cell>0.7</cell><cell>2.1974</cell><cell>2.2998</cell><cell>2.3261</cell><cell>2.3275</cell>
                </row>
                <row>
                    <cell>0.8</cell><cell>2.4872</cell><cell>2.6161</cell><cell>2.6493</cell><cell>2.6511</cell>
                </row>
                <row>
                    <cell>0.9</cell><cell>2.8159</cell><cell>2.9757</cell><cell>3.0170</cell><cell>3.1092</cell>
                </row>
                <row bottom="medium">
                    <cell>1.0</cell><cell>3.1875</cell><cell>3.3832</cell><cell>3.4238</cell><cell>3.4366</cell>
                </row>
            </tabular>      
        </table>

        <activity>
            <title>Euler's Method and Error</title>

            <introduction>
                <p>Consider the initial value problem
                    <md>
                        <mrow>y' \amp  = x + xy</mrow>
                        <mrow>y(0) \amp = 1.</mrow>
                    </md></p>
            </introduction>
            <task>
                <statement>
                    <p>Use separation of variables to solve the initial value problem.</p>
                </statement>
            </task>
            <task xml:id="firstlook04-task-exact">
                <statement>
                    <p>Compute <m>y(x)</m> for <m>x = 0, 0.2, 0.4, \ldots, 1</m>.</p>
                </statement>
            </task>
            <task xml:id="firstlook04-task-euler">
                <statement>
                    <p>Use Euler's method to approximate solutions to the initial value problem for <m>x = 0, 0.2, 0.4, \ldots, 1</m>.</p>
                </statement>
            </task>
            <task>
                <statement>
                    <p>Compare the exact values of the solution (<xref ref="firstlook04-task-exact" />) to the approximate values of the solution (<xref ref="firstlook04-task-euler" />) and comment on what happens as <m>x</m> varies from <m>0</m> to <m>1</m>.</p>
                </statement>
            </task>
        </activity>

    </subsection>

    <subsection xml:id="firstlook04-subsection-error-bound">
        <title>Finding an Error Bound</title>

        <p>To fully understand Euler's method, we will need to recall Taylor's theorem from calculus.</p>
            
        <theorem xml:id="firstlook04-theorem-taylor">
            <statement>
                <p>If <m>x \gt x_0</m>, then
                    <me>f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2!} (x - x_0)^2 + \cdots + \frac{f^{(n)}(\xi)}{n!}(x - x_0)^n,</me>
                where <m>\xi \in (x_0, x)</m>.<idx>Taylor's theorem</idx></p>
            </statement>
        </theorem> 
 
        <p>Given the initial value problem
            <md>
            <mrow>y' &amp; = f(t, y),</mrow>
            <mrow>y_0 &amp; = y (t_0),</mrow>
            </md>
        choose <m>t_1, t_2, \ldots, t_N</m> to be equally spaced on <m>[t_0, a]</m>, we can write
            <me>t_k = t_0 + kh,</me>
        where <m>h = (a - t_0)/N</m> and <m>k = 1, 2, \ldots, N</m>. Taylor's Theorem tells us that
            <me>y(t_{k + 1}) = y(t_k + h) = y(t_k) + y'(t_k) h + \frac{y''(t_k)}{2!} h^2 + \cdots.</me>
        If we know the values of <m>y</m> and its derivatives at <m>t_k</m>, then we can determine the value of <m>y</m> at <m>t_{k + 1}</m>.</p>


        <p>The simplest approximation can be obtained by taking the first two terms of the Taylor series. That is, we will use a linear approximation,
            <me>y_{k + 1} = y(t_{k+1}) \approx y(t_k) + y'(t_k) h = y(t_k) + f(t_k, y_k) h.</me>
        This gives us Euler's method,
            <md>
                <mrow>Y_0 &amp; = y(t_0)</mrow>
                <mrow>Y_1 &amp; = Y_0 + h f(t_0, Y_0)</mrow>
                <mrow>Y_2 &amp; = Y_1 + h f(t_1, Y_1)</mrow>
                <mrow>&amp; \vdots</mrow>
                <mrow>Y_{k+1} &amp; = Y_k + h f(t_k, Y_k).</mrow>
            </md>
        The terms that we are omitting, all contain powers of <m>h</m> of at least degree two. If <m>h</m> is small, then <m>h^n</m> for <m>n = 2, 3, \ldots</m> will be very small and these terms will not matter much.</p>
        
        <p>We can actually estimate the error incurred by Euler's method if we make use of Taylor's Theorem.</p>

        <theorem xml:id="firstlook04-theorem-euler-error-bound">
            <statement>
                <p>Let <m>y</m> be the unique solution to the initial value problem<idx><h>Euler's method</h><h>error bound</h></idx>
                    <md>
                        <mrow>y' &amp; =  f(t, y),</mrow>
                        <mrow>y(a) &amp; =  \alpha,</mrow>
                    </md>
                where <m>t \in [a, b]</m>. Suppose that <m>f</m> is continuous and  there exists a constant <m>L \gt 0</m> such that 
                    <me>|f(t, y_1) - f(t, y_2)| \leq L |y_1 - y_2|,</me>
                whenever <m>(t, y_1)</m> and <m>(t, y_2)</m> are in <m>D = [a, b] \times {\mathbb R}</m>. Also assume that there exists an <m>M</m> such that
                    <me>|y''(t)| \leq M</me>
                for all <m>t \in [a, b]</m>. If <m>Y_0, \ldots, Y_N</m> are the approximations generated by Euler's method for some positive integer <m>N</m>, then
                    <me>|y(t_i) - Y_i| \leq \frac{hM}{2L} [ e^{L(t_i - a)} - 1].</me></p>
            </statement>
        </theorem> 
            
        <p>The condition that  there exists a constant <m>L \gt 0</m> such that
            <me>|f(t, y_1) - f(t, y_2)| \leq L |y_1 - y_2|,</me>
        whenever <m>(t, y_1)</m> and <m>(t, y_2)</m> are in <m>D = [a, b] \times {\mathbb R}</m> is called a <term>Lipschitz condition</term><idx>Lipschitz condition</idx>. Many of the functions that we will consider satisfy such a condition. If the condition is satisfied, we can usually say a great deal about the function.</p>

        <table xml:id="firstlook04-table-euler-approximation-error-bound">
            <title>Error bound and actual error</title>
            <tabular halign="center" top="medium">
                <row bottom="medium">
                  <cell><m>k</m></cell><cell><m>t_k</m></cell><cell><m>Y_k</m></cell><cell><m>y_k = y(t_k)</m></cell><cell><m>|y_k - Y_k|</m></cell><cell>Estimated Error</cell>
                </row>
                    <row>
                        <cell>0</cell><cell>0.0</cell><cell>1.0000</cell><cell>1.0000</cell><cell>0.0000</cell><cell>0.0000</cell>
                    </row>
                    <row>
                        <cell>1</cell><cell>0.1</cell><cell>1.1000</cell><cell>1.1103</cell><cell>0.0103</cell><cell>0.0286</cell>
                    </row>
                    <row>
                        <cell>2</cell><cell>0.2</cell><cell>1.2200</cell><cell>1.2428</cell><cell>0.0228</cell><cell>0.0602</cell>
                    </row>
                    <row>
                        <cell>3</cell><cell>0.3</cell><cell>1.3620</cell><cell>1.3997</cell><cell>0.0377</cell><cell>0.0951</cell>
                    </row>
                    <row>
                        <cell>4</cell><cell>0.4</cell><cell>1.5282</cell><cell>1.5836</cell><cell>0.0554</cell><cell>0.1337</cell>
                    </row>
                    <row>
                        <cell>5</cell><cell>0.5</cell><cell>1.7210</cell><cell>1.7974</cell><cell>0.0764</cell><cell>0.1763</cell>
                    </row>
                    <row>
                        <cell>6</cell><cell>0.6</cell><cell>1.9431</cell><cell>2.0442</cell><cell>0.1011</cell><cell>0.2235</cell>
                    </row>
                    <row>
                        <cell>7</cell><cell>0.7</cell><cell>2.1974</cell><cell>2.3275</cell><cell>0.1301</cell><cell>0.2756</cell>
                    </row>
                    <row>
                        <cell>8</cell><cell>0.8</cell><cell>2.4872</cell><cell>2.6511</cell><cell>0.1639</cell><cell>0.3331</cell>
                    </row>
                    <row>
                        <cell>9</cell><cell>0.9</cell><cell>2.8159</cell><cell>3.1092</cell><cell>0.2033</cell><cell>0.3968</cell>
                    </row>
                    <row bottom="medium">
                        <cell>10</cell><cell>1.0</cell><cell>3.1875</cell><cell>3.4366</cell><cell>0.2491</cell><cell>0.4671</cell>
                    </row>
            </tabular>
        </table>

        <p>We can now compare the estimated error from our theorem to the actual error of our example. We first need to determine <m>M</m> and <m>L</m>. Since
            <me>|f(t, y_1) - f(t, y_2)| = |(y_1 + t) - (y_2 + t)| = |y_1 - y_2|,</me>
        we can take <m>L</m> to be one.  Since <m>y'' = 2e^t</m>, we can bound <m>y''</m> on the interval <m>[0, 1]</m> by <m>M = 2e</m>. Thus, we can bound the error by
            <me>|y(t_i) - Y_i| \leq \frac{hM}{2L} [ e^{L(t_i - a)} - 1] = 0.1e( e^{t_i } - 1)</me>
        for <m>h =0.1</m>. Our results are in <xref ref="firstlook04-table-euler-approximation-error-bound" />.</p>

    </subsection>

    <subsection xml:id="firstlook04-subsection-improving-euler">
        <title>Improving Euler's Method</title>

        <p>If we wish to improve upon Euler's method, we could add more terms of Taylor series. For example, we can obtain a more accurate approximation by using a quadratic Taylor polynomial,<idx><h>Euler's method</h><h>improved</h></idx>
            <me>y(t_1) \approx y_0 + f(t_0, y_0) h + \frac{y''(t_0)}{2} h^2.</me>
        However, we need to know <m>y''(t_0)</m> in order to use this approximation. Using the chain rule from multivariable calculus, we can differentiate both sides of <m>y' = f(t, y)</m> to obtain
            <me>y'' = \frac{\partial f}{\partial t} \frac{dt}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} = f_t + f f_y.</me>
        Thus, our approximation becomes
            <me>y(t_1) \approx y_0 + f(t_0, y_0) h + \frac{1}{2} \left( f_t(t_0, y_0) + f(t_0, y_0) f_y (t_0, y_0) \right) h^2.</me></p>
        
        <p>The problem is that some preliminary analytic work must be done. That is, before we can write a program to compute our solution, we must find <m>\partial f/\partial t</m> and <m>\partial f / \partial y</m>, although this is less of a problem with the availability of computer algebra systems such as <em>Sage</em>.</p>
            
        <p>Around 1900, two German mathematicians, Carle Runge and Martin Kutta, independently invented several numerical algorithms to solve differential equations. These methods, known as <term>Runge-Kutta methods</term><idx>Runge-Kutta method</idx>, estimate the higher-order terms of the Taylor series to find an approximation that does not depend on computing derivatives of <m>f(t, y)</m>.</p>

        <p>If we consider the initial value problem
            <md>
                <mrow>y' &amp; = f(t, y),</mrow>
                <mrow>y(t_0) &amp; = y_0,</mrow>
            </md>
        then
            <me>y(t_1) = y(t_0) + \int_{t_0}^{t_1} f(s, y(s)) \, ds</me>
        or
            <men xml:id="firstlook04-equation-runge-kutta-1">y_1 - y_0 = y(t_1) - y(t_0) = \int_{t_0}^{t_1} f(s, y(s)) \, ds</men>
        by the Fundamental Theorem of Calculus. In Euler's method, we approximate the right-hand side of <xref ref="firstlook04-equation-runge-kutta-1" /> by 
            <me>y_1 - y_0 = f(t_0, y_0) h.</me>
        In terms of the definite integral, this is simply a left-hand sum. In the <term>improved Euler's method</term><idx><h>Euler's method</h><h>improved</h></idx> or the <term>second-order Runge-Kutta method</term><idx><h>Runge-Kutta method</h><h>second-order</h></idx> we will estimate the right-hand side of <xref ref="firstlook04-equation-runge-kutta-1" /> using the trapezoid rule from calculus,
            <md>
                <mrow>y(t_1) - y(t_0) &amp; = \int_{t_0}^{t_1} f(s, y(s)) \, ds</mrow>
                <mrow>&amp; \approx \left( f(t_0, y_0) + f(t_1, y_1) \right) \frac{h}{2}.</mrow>
            </md>
        Thus, our algorithm becomes
            <men xml:id="firstlook04-equation-runge-kutta-2">y_1 = y_0 + \left( f(t_0, y_0) + f(t_1, y_1) \right) \frac{h}{2}.</men>
        However, we have a problem since <m>y_1</m> appears in the right-hand side of our approximation. To get around this difficulty, we will replace <m>y_1</m> in the right-hand side of <xref ref="firstlook04-equation-runge-kutta-2" /> with the Euler approximation for <m>y_1</m>. Thus,
            <me>y_1 = y_0 + \left( f(t_0, y_0) + f(t_1, y_0 + f(t_0, y_0) h) \right) \frac{h}{2}.</me></p>

        <p>To understand that the second-order Runge-Kutta method is actually an improvement over the traditional Euler's method, we will need to use the Taylor approximation for a function of two variables. Let us assume that <m>f(x,y)</m> is defined on some rectangle and that all of the derivatives of <m>f</m> are continuously differentiable. Then 
            <md>
                <mrow>f(x + h, y + k) &amp; = f(x, y) + h \frac{\partial}{\partial x} f(x, y) + k \frac{\partial}{\partial y}f(x, y)</mrow>
                <mrow>&amp; + \frac{1}{2!} \left( h^2 \frac{\partial^2}{\partial^2 x} f(x, y) + hk \frac{\partial^2}{\partial x \partial y}f(x, y) + k^2 \frac{\partial^2}{\partial^2 y}f(x, y) \right)</mrow>
                <mrow>&amp; + \frac{1}{3!} \left( h^3 \frac{\partial^3}{\partial^3 x} f(x, y)  + 3h^2k \frac{\partial^3}{\partial^2 x \partial y}f(x, y) \right.</mrow>
                <mrow>&amp; + \left. hk^2 \frac{\partial^3}{\partial x\partial^2 y}f(x, y) + k^3 \frac{\partial^3}{\partial^3 y} f(x, y) \right) + \cdots.</mrow>
            </md>
        As in the case of the single variable Taylor series, we can write a Taylor polynomial if the Taylor series is truncated,
            <md>
                <mrow>f(x + h, y + k) &amp; =  \sum_{n = 0}^{N} \frac{1}{n!} \left(  h \frac{\partial}{\partial x} + k \frac{\partial}{\partial y} \right)^n f(x, y)</mrow>
                <mrow>&amp; + \frac{1}{(N+1)!} \left(  h \frac{\partial}{\partial x} + k \frac{\partial}{\partial y} \right)^{N+1} f(\overline{x}, \overline{y} ),</mrow>
            </md>
        where the second term is the remainder term and <m>(\overline{x}, \overline{y} )</m> lies on the line segment joining <m>(x, y)</m> and <m>(x + h, y + k)</m>.</p>
    
        <p>In the Improved Euler's Method, we adopt a formula
            <me>y(t + h) = y(t) + w_1 F_1 + w_2 F_2,</me>
        where
            <md>
                <mrow>F_1 &amp; = h f(t, y)</mrow>
                <mrow>F_2 &amp; = h f(t + \alpha h, y + \beta F_1).</mrow>
            </md>
        That is,
            <men xml:id="firstlook04-equation-runge-kutta-3">y(t + h) = y(t) + w_1 h f(t, y) + w_2 h f(t + \alpha h, y + \beta h f(t, y)).</men>
        The idea is to choose the constants <m>w_1</m>, <m>w_2</m>, <m>\alpha</m>, and <m>\beta</m> as accurately as possible in order to duplicate as many terms as possible in the Taylor series
            <men xml:id="firstlook04-equation-runge-kutta-4">y(t + h) = y(t) + h y'(t) + \frac{h^2}{2!} y''(t) + \frac{h^3}{3!} y''(t) + \cdots.</men>
        We can make equations <xref ref="firstlook04-equation-runge-kutta-3" /> and <xref ref="firstlook04-equation-runge-kutta-4" /> agree if we choose <m>w_1 = 1</m> and <m>w_2 = 0</m>. Since <m>y' = f</m>, we obtain Euler's method.</p>

        <p>If we are more careful about choosing our parameters, we can obtain agreement up through the <m>h^2</m> term. If we use the two variable Taylor series to expand <m>f(t + \alpha h, y + \beta h f)</m>, we have
            <me>f(t + \alpha h, y + \beta h f) = f + \alpha h f_t + \beta h f f_y + {\mathcal O}(h^2),</me>
        where <m>{\mathcal O}(h^2)</m> means that of the subsequent terms have a factor of <m>h^n</m> with <m>n \geq 2</m>. Using this expression, we obtain a new form for <xref ref="firstlook04-equation-runge-kutta-3" />, 
            <men xml:id="firstlook04-equation-runge-kutta-5">y(t + h) = y(t) + (w_1 + w_2) hf + \alpha w_2 h^2 f_t + \beta w_2 h^2 f f_y + {\mathcal O}(h^3).</men>
        Since <m>y'' = f_t + f_y f</m> by the chain rule, we can rewrite <xref ref="firstlook04-equation-runge-kutta-4" /> as
            <men xml:id="firstlook04-equation-runge-kutta-6">y(t + h) = y(t) + h f + \frac{h^2}{2!}( f_t +  f f_y) + {\mathcal O}(h^3).</men>
        We can make equations <xref ref="firstlook04-equation-runge-kutta-5" /> and <xref ref="firstlook04-equation-runge-kutta-6" /> agree up through the quadratic terms if we require that
            <md>
                <mrow>w_1 + w_2 &amp; = 1,</mrow>
                <mrow>\alpha w_2 &amp; = \frac{1}{2},</mrow>
                <mrow>\beta w_2 &amp; = \frac{1}{2}.</mrow>
            </md>
        If we choose <m>\alpha = \beta = 1</m> and <m>w_1 = w_2 = 1/2</m>, these equations are satisfied, and we obtain the improved Euler's method
            <me>y(t + h) = y(t) + \frac{h}{2} f(t, h) + \frac{h}{2} f(t + h, y  + h f(t, y)).</me></p>

        <p>The improved Euler's method or the second-order Runge-Kutta method is a more sophisticated algorithm that is less prone to error due to the step size <m>h</m>. Euler's method is based on truncating the Taylor series after the linear term. Since
            <me>y(t + h) = y(t) + h y'(t) + {\mathcal O}(h^2),</me>
        we know that the error depends on <m>h</m>. On the other hand, the error for the improved Euler's method depends on <m>h^2</m>, since
            <me>y(t + h) = y(t) + h y'(t) + \frac{h^2}{2!} y''(t) + {\mathcal O}(h^3).</me></p>

        <p>If we use Simpson's rule to estimate the integral in 
            <me>y(t_1) - y(t_0) = \int_{t_0}^{t_1} f(s, y(s)) \, ds,</me>
        we can improve our accuracy up to <m>h^4</m>.  The idea is exactly the same, but the algebra becomes much more tedious. This method is known as the Runge-Kutta method of order 4 and is given by<idx><h>Runge-Kutta method</h><h>order 4</h></idx>
            <me>y(t + h) = y(t) + \frac{1}{6} (F_1 + 2 F_2 + 2 F_3 + F_4),</me>
        where
            <md>
                <mrow>F_1 &amp; = h f(t, y)</mrow>
                <mrow>F_2 &amp; = hf\left( t + \frac{1}{2} h, y + \frac{1}{2} F_1 \right)</mrow>
                <mrow>F_3 &amp; = hf\left(  t + \frac{1}{2} h, y + \frac{1}{2} F_2 \right)</mrow>
                <mrow>F_4 &amp; = hf(t + h, y + F_3).</mrow>
            </md></p>

    </subsection>

    <subsection xml:id="firstlook04-subsection-important-lessons">
        <title>Important Lessons</title>
        
       <p><ul>
        
            <li><p>We can use Euler's method to find an approximate solution to the initial value problem
                <md>
                    <mrow>y' &amp; = f(t, y),</mrow>
                    <mrow>y(a) &amp; = \alpha</mrow>
                </md>
            on an interval <m>[a, b]</m>.  If we wish to find approximations at <m>N</m> equally spaced points <m>t_1, \ldots, t_N</m>, where <m>h = (b-a)/N</m> and <m>t_i = a + i h</m>, our approximations should be
                <md>
                    <mrow>Y_0 &amp; = \alpha,</mrow>
                    <mrow>Y_1 &amp; = Y_0 + h f(\alpha, Y_0),</mrow>
                    <mrow>Y_2 &amp; = Y_1 + h f(t_1, Y_1,)</mrow>
                    <mrow>&amp; \vdots</mrow>
                    <mrow>Y_{k+1} &amp; = Y_k + h f(t_k, Y_k),</mrow>
                    <mrow>Y_N &amp; = Y_{N-1} + h f(t_{N-1}, Y_{N-1}).</mrow>
                </md>
                In practice, no one uses Euler's method. The Runge-Kutta methods are better algorithms.</p></li>

            <li><p>Taylor's Theorem is a very useful tool for studying differential equations. If <m>x \gt x_0</m>, then
                <me>f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2!} (x - x_0)^2 + \cdots + \frac{f^{(n)}(\xi)}{n!}(x - x_0)^n,</me>
            where <m>\xi \in (x_0, x)</m>.</p></li>

            <li><p>Error analysis rate of convergence is very important for any numerical algorithm. Our approximation is more accurate for smaller values of <m>h</m>. Under reasonable conditions we can also bound the error by
                <me>|y(t_i) - Y_i| \leq \frac{hM}{2L} [ e^{L(t_i - a)} - 1],</me>
            where <m>y</m> is the unique solution to the initial value problem
                <md>
                    <mrow>y' &amp; = f(t, y),</mrow>
                    <mrow>y(a) &amp; = \alpha.</mrow>
                </md></p></li>

            <li><p>The condition that there exists a constant <m>L \gt 0</m> such that 
                <me>|f(t, y_1) - f(t, y_2)| \leq L |y_1 - y_2|,</me>
            whenever <m>(t, y_1)</m> and <m>(t, y_2)</m> are in <m>D = [a, b] \times {\mathbb R}</m> is called a <term>Lipschitz condition</term>.</p></li>

            <li><p>Using Taylor series, we can develop better numerical algorithms to compute solutions of differential equations. The Runge-Kutta methods are an important class of these algorithms.</p></li>

            <li><p>The improved Euler's method is given by
                <me>y(t + h) = y(t) + \frac{h}{2} f(t, h) + \frac{h}{2} f(t + h, y + h f(t, y))</me>
            with the error bound depending on <m>h^2</m>.</p></li>

            <li><p>The Runge-Kutta method of order 4 is given by
                <me>y(t + h) = y(t) + \frac{1}{6} (F_1 + 2 F_2 + 2 F_3 + F_4),</me>
            where
                <md>
                    <mrow>F_1 &amp; = h f(t, y)</mrow>
                    <mrow>F_2 &amp; = hf\left( t + \frac{1}{2} h, y + \frac{1}{2} F_1 \right)</mrow>
                    <mrow>F_3 &amp; = hf\left(  t + \frac{1}{2} h, y + \frac{1}{2} F_2 \right)</mrow>
                    <mrow>F_4 &amp; = hf(t + h, y + F_3)</mrow>
                </md>
            with the error bound depending on <m>h^4</m>.</p></li>

        </ul></p>

    </subsection>


    <reading-questions xml:id="reading-questions-firstlook04">

        <exercise xml:id="reading-questions-firstlook04-1" label="rq-firstlook04-taylor">
            <statement>
                <p>We can use Taylor polynomials to approximate a function <m>f(x)</m> near a point <m>x_0</m>. Explain why this approximation can only be expected to be accurate near <m>x_0</m>.</p>
            </statement>
            <response/>
        </exercise>

        <exercise xml:id="reading-questions-firstlook04-2" label="rq-firstlook04-euler">
            <statement>
                <p>Should we always use Euler's method when approximating a solution to an initial value problem?  Why or why not?</p>
            </statement>
            <response/>
        </exercise>

    </reading-questions>


    <exercises xml:id="exercises-firstlook04"  filenamebase="firstlook04">
        <title>Exercises</title>

        <exercisegroup cols="2" xml:id="firstlook04-exercises-euler">
            <title>Finding Solutions</title>
            <introduction><p>For each of the initial value problem
                <md>
                    <mrow>y' &amp; = f(t, y),</mrow>
                    <mrow>y(t_0) &amp; = y_0</mrow>
                </md>
                in <xref ref="firstlook04-exercises-euler"/>,
                <ol>
                    <li><p>Write the Euler's method iteration <m>Y_{k+1} = Y_k + h f(t_k, Y_k)</m> for the given problem, identifying the values <m>t_0</m> and <m>y_0</m>.</p></li>

                    <li><p>Using a step size of <m>h = 0.1</m>, compute the approximations for <m>Y_1</m>, <m>Y_2</m>,
                    and <m>Y_3</m>.</p></li>

                    <li><p>Solve the problem analytically if possible. If it is not possible for you to find the analytic solution, use <em>Sage</em>.</p></li>

                    <li><p>Use the results of (b) and (c), to construct a table of errors for <m>Y_i - y_i</m> for <m>i = 1, 2, 3</m>.</p></li>

                </ol></p></introduction>
            <exercise>
                <statement>
                    <p><m>y' = -2y</m>, <m>y(0) = 0</m></p>
                </statement>
            </exercise>
            <exercise>
                <statement>
                    <p><m>y' = ty</m>, <m>y(0) = 1</m></p>
                </statement>
            </exercise>
            <exercise>
                <statement>
                    <p><m>y' = y^3</m>, <m>y(0) = 1</m></p>
                </statement>
            </exercise>
            <exercise>
                <statement>
                    <p><m>y' = y</m>, <m>y'(0) = 1</m></p>
                </statement>
            </exercise>
            <exercise>
                <statement>
                    <p><m>y' = y + t</m>, <m>y'(0) = 2</m></p>
                </statement>
                <hint>
                    <p>This equation is a first-order linear equation (<xref ref="firstlook05"/>), but it is possible to find the analytic solution using <em>Sage</em> (<xref ref="firstlook02-subsection-sage"/>).</p>
                </hint>
            </exercise>
            <exercise>
                <statement>
                    <p><m>y' = 1/y</m>, <m>y'(0) = 2</m></p>
                </statement>
            </exercise>
        </exercisegroup>
        
        <exercise>
            <statement>
                <p>Consider the differential equation
                    <me>\frac{dy}{dt} = 3y - 1</me>
                with initial value <m>y(0) = 2</m>.
                    <ol>

                        <li><p>Find the exact solution of the initial value problem.</p></li>

                        <li><p>Use Euler's method with step size <m>h = 0.5</m> to approximate the solution to the initial value problem on the interval <m>[0,2]</m>  Your solution should include a table of approximate values of the dependent variable as well as the exact values of the dependent variable.  Make sure that your approximations are accurate to <em>four</em> decimal places.</p></li>

                        <li><p>Sketch the graph of the approximate and exact solutions.</p></li>

                        <li><p>Use the error bound theorem (<xref ref="firstlook04-theorem-euler-error-bound" />) to estimate the error at each approximation.  Your solution should include a table of approximate values of the dependent variable the exact values of the dependent variable, the error estimates, and the actual error.  Make sure that your approximations are accurate to <em>four</em> decimal places.</p></li>

                    </ol></p>
            </statement>

        </exercise>

        <exercise>
            <statement>
                <p>In this series of exercises, we will prove the error bound theorem for Euler's method (<xref ref="firstlook04-theorem-euler-error-bound" />).
                    <ol>

                        <li><p>Use Taylor's Theorem to show that for all <m>x \geq -1</m> and any positive <m>m</m>,
                            <me>0 \leq (1 + x)^m \leq e^{mx}.</me></p></li>

                        <li><p>Use part (1) and geometric series to prove the following statement: If <m>s</m> and <m>t</m> are positive real numbers, and <m>\{ a_i \}_{i = 0}^k</m> is a sequence satisfying
                            <md>
                                <mrow>a_0 &amp; \geq -\frac{t}{s}</mrow>
                                <mrow>a_{i+1} &amp; \leq (1 + s) a_i + t, \text{ for }i = 0, 1, 2, \ldots, k,</mrow>
                            </md>
                        then
                            <me>a_{i + 1} \leq e^{(i + 1)s} \left(  \frac{t}{s} + a_0 \right) - \frac{t}{s}.</me></p></li>

                        <li><p>When <m>i = 0</m>, <m>y(t_0) = Y_0 = \alpha</m> and the theorem is true.  Use Euler's method and Taylor's Theorem to show that 
                            <me>|y_{i + 1} - Y_{i + 1} | \leq |y_i - Y_i| + h |f(t_i, y_i) - f(t_i, Y_i)| + \frac{h^2}{2} |y''(\xi_i)|</me>
                        where <m>\xi_i \in (t_i, t_{i+1})</m> and <m>y_k = y(t_k)</m>.</p></li>

                        <li><p>Show that 
                            <me>|y_{i + 1} - Y_{i + 1} | \leq  |y_i - Y_i| (1 + hL) + \frac{Mh^2}{2}. </me></p></li>

                        <li><p>Let <m>a_j = |y_j - Y_j|</m> for <m>j = 0, 1, \ldots, N</m>, <m>s = hL</m>, and <m>t = M h^2/2</m> and apply part (2) to show
                            <me>|y_{i + 1} - Y_{i + 1} | \leq e^{(1 + i)hL} \left( |y_0 - Y_0| + \frac{M h^2 }{2hL} \right) - \frac{M h^2}{2hL}. </me>
                        and derive that
                            <me>|y_{i + 1} - Y_{i + 1} | \leq \frac{M h }{2L} \left( e^{(t_{i+ 1} - a)L} - 1\right)</me>
                        for each <m>i = 0, 1, \ldots, N-1</m>.</p></li>

                    </ol></p>
            </statement>
            <hint>
                <p>Hints for part (2):<ul>
                    <li><p>For fixed <m>i</m> show that 
                        <md>
                            <mrow>a_{i + 1} &amp; \leq (1 + s)a_i + t</mrow>
                            <mrow>&amp; \leq (1 + s)[(1 + s)a_{i - 1}+ t] + t</mrow>
                            <mrow>&amp; \leq (1 + s)\{ (1 + s)[(1 + s) a_{i - 2} + t]+ t\} + t</mrow>
                            <mrow>&amp; \vdots</mrow>
                            <mrow>&amp; \leq (1 + s)^{i+1}a_0 + [1 + (1 + s) + (1+s)^2 + \cdots + (1 + s)^i]t.</mrow>
                        </md></p></li>

                    <li><p>Now use a geometric sum to show that
                        <me>a_{i + 1} \leq (1 + s)^{i + 1} a_0 + \frac{t}{s}[(1 + s)^{i + 1} - 1] = (1 + s)^{i + 1} \left( \frac{t}{s} + a_0 \right) - \frac{t}{s}.</me></p></li>

                    <li><p>Apply part (1) to derive
                        <me>a_{i + 1} \leq e^{(i + 1)s} \left( \frac{t}{s} + a_0 \right) - \frac{t}{s}.</me></p></li>

                </ul></p>
            </hint>
        </exercise>

    </exercises>

    <subsection number="no" xml:id="firstlook04-subsection-sage-project">
        <title>Sage<mdash />Numerical Routines for solving ODEs</title>

        <p>Not all differential equations can be solved using algebra and calculus even if we are very clever. If we encounter an equation that we cannot solve or use <em>Sage</em> to solve, we must resort to numerical algorithms like Euler's method or one of the Runge-Kutta methods, which are best implemented using a computer. Fortunately, <em>Sage</em> has some very good numerical solvers. <em>Sage</em> will need to know the following to solve a differential equation:
            <ul>

                <li><p>An abstract function,</p></li>

                <li><p>A differential equation, <em>including an initial condition</em>,</p></li>

                <li><p>A <em>Sage</em> command to solve the equation.</p></li>

        </ul></p>

        <p>Suppose we wish to solve the initial value problem <m>dy/dx = x + y</m>, <m>y(0) = 1</m>. We can use <em>Sage</em> to find an algebraic solution.</p>

        <sage>
            <input>
            y = function('y')(x)
            de = diff(y,x) == x + y
            solution = desolve(de, y, ics = [0,1])
            solution.show()
            plot(solution, x, 0, 5)
            </input>
            <output>
            </output>
        </sage> 

        <p>We can also use Euler's method to find a solution for our initial value problem.</p>

       <sage>
            <input>
            x,y = PolynomialRing(RR, 2, "xy").gens() #declare x, y as generators of a polynomial ring
            eulers_method(x + y, 0,1, 0.5, 5)
            </input>
            <output>
            </output>
        </sage> 

        <p>The syntax of <c>eulers_method</c> for the inital value problem
            <md>
                <mrow>y \amp = f(x,y)</mrow>
                <mrow>f(x_0) \amp = y_0</mrow>
            </md>
        with step size <m>h</m> on the interval <m>[x_0, x_1]</m> is <c>eulers_method(f, x0, y0, h, x1)</c> Notice that we obtained a table of values. However, we can use the <c>line</c> command from <em>Sage</em> to plot the values <m>(x, y)</m>.</p>

        <sage>
            <input>
            x,y = PolynomialRing(RR, 2, "xy").gens()
            pts = eulers_method(x + y, 0, 1, 1/2, 4.5, algorithm="none")
            p = list_plot(pts, color="red")
            p += line(pts)
            p
            </input>
            <output>
            </output>
        </sage> 

        <p>As we pointed out, <c>eulers_method</c> is not very sophisticated. We have to use a very small step size to get good accuracy, and the method can generate errors if we are not careful. Fortunately, <em>Sage</em> has much better algorithms for solving initial value problems.  One such algorithm is <c>desolve_rk4</c>, which implements the fourth order Runge-Kutta method.</p>

        <sage>
            <input>
            x,y = var('x y')
            desolve_rk4(x + y, y, ics=[0, 1], end_points = 5, step = 0.5)
            </input>
            <output>
            </output>
        </sage> 

        <p>Again, we just get a list of points.  However, <c>desolve_rk4</c> has some nice built-in graphing utilities.</p>

        <sage>
            <input>
            x,y = var('x y')
            p = desolve_rk4(x + y, y, ics = [0,1], ivar = x, output = 'slope_field', end_points = [0, 5], thickness = 2, color = 'red')
            p
            </input>
            <output>
            </output>
        </sage> 

        <p>Write the <em>Sage</em> commands to compare the graphs obtained using <c>eulers_method</c> and <c>desolve</c> with the exact solution.</p>


        <sage>
            <input>
            </input>
            <output>
            </output>
        </sage>

        <p>Not only is <c>desolve_rk4</c> more accurate, it is much easier to use. For more information, see <url href="http://www.sagemath.org/doc/reference/calculus/sage/calculus/desolvers.html" visual="www.sagemath.org/doc/reference/calculus/sage/calculus/desolvers.html" />.</p>

    </subsection>

</section>



